use std::sync::{Arc, OnceLock};

use async_trait::async_trait;
use fastcrypto::hash::HashFunction;
use fleek_blake3 as blake3;
use lightning_interfaces::prelude::*;
use lightning_interfaces::types::{Block, Epoch, Event, Metadata, NodeIndex, TransactionRequest};
use lightning_interfaces::ExecutionEngineSocket;
use lightning_utils::application::QueryRunnerExt;
use narwhal_crypto::DefaultHashFunction;
use narwhal_executor::ExecutionState;
use narwhal_types::{BatchAPI, BatchDigest, ConsensusOutput, Transaction};
use serde::{Deserialize, Serialize};
use tokio::sync::{mpsc, Notify};
use tracing::{error, info};

pub type Digest = [u8; 32];

#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct AuthenticStampedParcel {
    pub transactions: Vec<Transaction>,
    pub last_executed: Digest,
    pub epoch: Epoch,
}

impl ToDigest for AuthenticStampedParcel {
    fn transcript(&self) -> TranscriptBuilder {
        panic!("We don't need this here");
    }

    fn to_digest(&self) -> Digest {
        let batch_digest =
            BatchDigest::new(DefaultHashFunction::digest_iterator(self.transactions.iter()).into());

        let mut bytes = Vec::new();
        bytes.extend_from_slice(&(self.transactions.len() as u32).to_le_bytes());
        bytes.extend_from_slice(&batch_digest.0);
        bytes.extend_from_slice(&self.last_executed);

        blake3::hash(&bytes).into()
    }
}

/// A message an authority sends out attest that an Authentic stamp parcel is accurate. When an edge
/// node gets 2f+1 of these it commits the transactions in the parcel
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct CommitteeAttestation {
    /// The digest we are attesting is correct
    pub digest: Digest,
    /// We send random bytes with this message so it gives it a unique hash and differentiates it
    /// from the other committee members attestation broadcasts
    pub node_index: NodeIndex,
    pub epoch: Epoch,
}

pub struct Execution<Q: SyncQueryRunnerInterface, NE: Emitter> {
    /// Managing certificates generated by narwhal.
    executor: ExecutionEngineSocket,
    /// Used to signal internal consensus processes that it is time to reconfigure for a new epoch
    reconfigure_notify: Arc<Notify>,
    /// Used to send payloads to the edge node consensus to broadcast out to other nodes
    tx_narwhal_batches: mpsc::Sender<(AuthenticStampedParcel, bool)>,
    /// Query runner to check application state, mainly used to make sure the last executed block
    /// is up to date from time we were an edge node
    query_runner: Q,
    /// Notifications emitter
    notifier: NE,
    /// Send the event to the RPC
    event_tx: OnceLock<mpsc::Sender<Vec<Event>>>,
}

impl<Q: SyncQueryRunnerInterface, NE: Emitter> Execution<Q, NE> {
    pub fn new(
        executor: ExecutionEngineSocket,
        reconfigure_notify: Arc<Notify>,
        tx_narwhal_batches: mpsc::Sender<(AuthenticStampedParcel, bool)>,
        query_runner: Q,
        notifier: NE,
    ) -> Self {
        Self {
            executor,
            reconfigure_notify,
            tx_narwhal_batches,
            query_runner,
            notifier,
            event_tx: OnceLock::new(),
        }
    }

    // Returns true if the epoch changed
    pub(crate) async fn submit_batch(&self, payload: Vec<Transaction>, digest: Digest) -> bool {
        let transactions = payload
            .into_iter()
            .filter_map(|txn| TransactionRequest::try_from(txn.as_ref()).ok())
            .collect::<Vec<_>>();

        if transactions.is_empty() {
            return false;
        }

        let block = Block {
            transactions,
            digest,
        };

        let archive_block = block.clone();

        // Unfailable
        let response = self.executor.run(block).await.unwrap();
        info!("Consensus submitted new block to application");

        match self.event_tx.get() {
            Some(tx) => {
                if let Err(e) = tx
                    .send(
                        response
                            .txn_receipts
                            .iter()
                            .filter_map(|r| r.event.clone())
                            .collect(),
                    )
                    .await
                {
                    error!("We could not send a message to the RPC: {e}");
                }
            },
            None => {
                error!("Once Cell not initialized, this is a bug");
            },
        }

        let change_epoch = response.change_epoch;
        self.notifier.new_block(archive_block, response);

        if change_epoch {
            let epoch_number = self.query_runner.get_current_epoch();
            let epoch_hash = self
                .query_runner
                .get_metadata(&Metadata::LastEpochHash)
                .expect("We should have a last epoch hash")
                .maybe_hash()
                .expect("We should have gotten a hash, this is a bug");

            self.notifier.epoch_changed(epoch_number, epoch_hash);
        }

        change_epoch
    }

    pub fn shutdown(&self) {
        self.executor.downgrade();
    }

    pub fn set_event_tx(&self, tx: mpsc::Sender<Vec<Event>>) {
        self.event_tx.set(tx).unwrap();
    }
}

#[async_trait]
impl<Q: SyncQueryRunnerInterface, NE: Emitter> ExecutionState for Execution<Q, NE> {
    async fn handle_consensus_output(&self, consensus_output: ConsensusOutput) {
        for (cert, batches) in consensus_output.batches {
            let current_epoch = self.query_runner.get_current_epoch();
            if cert.epoch() != current_epoch {
                // If the certificate epoch does not match the current epoch in the application
                // state do not execute this transaction, This could only happen in
                // certain race conditions at the end of an epoch and we need this to ensure all
                // nodes execute the same transactions
                continue;
            }

            if !batches.is_empty() {
                let mut batch_payload =
                    Vec::with_capacity(batches.iter().fold(0, |acc, batch| acc + batch.size()));

                for batch in batches {
                    for tx_bytes in batch.transactions() {
                        if let Ok(tx) = TransactionRequest::try_from(tx_bytes.as_ref()) {
                            if !self.query_runner.has_executed_digest(tx.hash()) {
                                batch_payload.push(tx_bytes.to_owned());
                            }
                        }
                    }
                }

                if batch_payload.is_empty() {
                    continue;
                }

                // We have batches in the payload send them over broadcast along with an attestion
                // of them
                let last_executed = self.query_runner.get_last_block();
                let parcel = AuthenticStampedParcel {
                    transactions: batch_payload.clone(),
                    last_executed,
                    epoch: current_epoch,
                };

                let epoch_changed = self.submit_batch(batch_payload, parcel.to_digest()).await;

                if let Err(e) = self.tx_narwhal_batches.send((parcel, epoch_changed)).await {
                    // This shouldn't ever happen. But if it does there is no critical tasks
                    // happening on the other end of this that would require a
                    // panic
                    error!("Narwhal failed to send batch payload to edge consensus: {e:?}");
                }

                // Submit the batches to application layer and if the epoch changed reset last
                // executed
                if epoch_changed {
                    self.reconfigure_notify.notify_waiters();
                }
            }
        }
    }

    async fn last_executed_sub_dag_index(&self) -> u64 {
        0
    }
}
